{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.6\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkrislara\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "from PIL import Image\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "import numpy as np\n",
    "from pytorchtools import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(hash(\"setting random seeds\") % 2**32 - 1)\n",
    "np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n",
    "torch.manual_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n",
    "torch.cuda.manual_seed_all(hash(\"so runs are repeatable\") % 2**32 - 1)\n",
    "\n",
    "device =torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_transform(size,scale):\n",
    "  return transforms.Compose([\n",
    "                    transforms.Resize(size//scale,Image.BICUBIC),\n",
    "                    transforms.ToTensor(),           \n",
    "  ])\n",
    "def target_transform():\n",
    "  return transforms.ToTensor()\n",
    "\n",
    "def load_img(path):\n",
    "  img=Image.open(path)\n",
    "  yuv=img.copy()\n",
    "  yuv=yuv.convert('YCbCr')\n",
    "  y,u,v=yuv.split()\n",
    "  img=np.asarray(img)\n",
    "  return y,u,v,img\n",
    "\n",
    "#inherit dataset for collective dataset\n",
    "class SRDataset(data.Dataset):\n",
    "  def __init__(self,root_dir,input_transform=None,target_transform=None,\n",
    "               fetch=\"train\"):\n",
    "    self.root_dir=root_dir\n",
    "    self.input_transform=input_transform\n",
    "    self.target_transform=target_transform\n",
    "    self.fetch=fetch\n",
    "    self.image_file_names=os.listdir(os.path.join(self.root_dir,\n",
    "                                                  self.fetch.lower(),\"HR\"))\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.image_file_names)\n",
    "  \n",
    "  def __getitem__(self,idx):\n",
    "    if torch.is_tensor(idx):\n",
    "      idx=idx.tolist()\n",
    "    \n",
    "    self.image_file_names.sort()\n",
    "    HR,u,v,HRimg=load_img(os.path.join(self.root_dir,self.fetch.lower(),\n",
    "                                       \"HR\",self.image_file_names[idx]))\n",
    "    LR=HR.copy()\n",
    "    if self.input_transform:\n",
    "      LR=self.input_transform(LR)\n",
    "      u=self.input_transform(u)\n",
    "      v=self.input_transform(v)\n",
    "    if self.target_transform:\n",
    "      HR=self.target_transform(HR)\n",
    "    sample={'LR':LR,'HR':HR}\n",
    "    \n",
    "   \n",
    "    if self.fetch=='val':\n",
    "      return sample,u,v,HRimg\n",
    "    else:\n",
    "      return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/home/htic/SRDataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config =dict(\n",
    "    epochs=1000,\n",
    "    momentum=0.9,\n",
    "    batch_size=2,\n",
    "    learning_rate=0.0001,\n",
    "    gradient_clip=1.0,\n",
    "    dataset=\"SRdata\",\n",
    "    architecture=\"SRHW\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model pipeline\n",
    "def model_pipeline(config=config):\n",
    "  with wandb.init(project=\"SRHW\",config=config):\n",
    "    config=wandb.config\n",
    "    model,train_loader,steps_per_epoch,val_loader,criterion,optimizer=make(config)\n",
    "    print(\"Model Created\")\n",
    "    early_stopping_patience=100\n",
    "    ckpt='checkpoint.pt'\n",
    "    model=train(model,train_loader,steps_per_epoch,val_loader,criterion,optimizer,\n",
    "                early_stopping_patience,config,schedule=False,retrain=True,ckpt=ckpt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make\n",
    "def make(config):\n",
    "  train_data,train_loader,steps_per_epoch=get_data(path,input_transform(128,2),\n",
    "                                   target_transform(),'train',batch_size=config.batch_size)\n",
    "  val_data,val_loader,_=get_data(path,input_transform(256,2),\n",
    "                               target_transform(),'val')\n",
    "\n",
    "  model=SRHW().to(device)\n",
    "\n",
    "  criterion=nn.L1Loss()\n",
    "  optimizer= torch.optim.Adam(model.parameters(),lr=config.learning_rate)\n",
    "\n",
    "  return model,train_loader,steps_per_epoch,val_loader,criterion,optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_data and make_loader\n",
    "def get_data(dir,input_transform,target_transform,fetch=\"train\",\n",
    "             batch_size=2,shuffle=True,num_workers=0):\n",
    "  dataset=SRDataset(root_dir=dir, input_transform=input_transform,\n",
    "                    target_transform=target_transform,fetch=fetch)\n",
    "  if fetch=='val':\n",
    "    batch_size=len(dataset)\n",
    "  loader=data.DataLoader(dataset,batch_size=batch_size,shuffle=shuffle,\n",
    "                         num_workers=num_workers)\n",
    "  steps_per_epoch=len(dataset)//batch_size\n",
    "  return dataset,loader,steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "class SRHW(nn.Module):\n",
    "    def __init__(self,upscale=2):\n",
    "        super(SRHW,self).__init__()\n",
    "        self.Conv1=nn.Conv2d(1,32,3,padding=(1,1),bias=False)\n",
    "        nn.init.uniform_(self.Conv1.weight)\n",
    "        self.DWConv1=nn.Conv2d(32,32,(1,5),padding=(0,2),groups=32,bias=False)\n",
    "        nn.init.uniform_(self.DWConv1.weight)\n",
    "        self.PWConv1=nn.Conv2d(32,16,1,bias=False)\n",
    "        nn.init.uniform_(self.PWConv1.weight)\n",
    "        self.DWConv2=nn.Conv2d(16,16,(1,5),padding=(0,2),groups=16,bias=False)\n",
    "        nn.init.uniform_(self.DWConv2.weight)\n",
    "        self.PWConv2=nn.Conv2d(16,32,1,bias=False)\n",
    "        nn.init.uniform_(self.PWConv2.weight)\n",
    "        self.DWConv3=nn.Conv2d(32,32,3,padding=(1,1),groups=32,bias=False)\n",
    "        nn.init.uniform_(self.DWConv3.weight)\n",
    "        self.PWConv3=nn.Conv2d(32,16,1,bias=False)\n",
    "        nn.init.uniform_(self.PWConv3.weight)\n",
    "        self.DWConv4=nn.Conv2d(16,16,3,padding=(1,1),groups=16,bias=False)\n",
    "        nn.init.uniform_(self.DWConv4.weight)\n",
    "        self.PWConv4=nn.Conv2d(16,upscale**2,1,bias=False)\n",
    "        nn.init.uniform_(self.PWConv4.weight)\n",
    "        self.PS=nn.PixelShuffle(upscale)\n",
    "        self.relu=nn.ReLU()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.Conv1(x)\n",
    "        res=self.relu(x)\n",
    "        res=self.relu(self.PWConv1(self.DWConv1(res)))\n",
    "        res=self.PWConv2(self.DWConv2(res))\n",
    "        x=x+res\n",
    "        x=self.relu(x)\n",
    "        x=self.relu(self.PWConv3(self.DWConv3(x)))\n",
    "        x=self.PWConv4(self.DWConv4(x))\n",
    "        x=self.PS(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train with early stopping\n",
    "def train(model,train_loader,steps_per_epoch,val_loader,criterion,optimizer,patience,\n",
    "          config,schedule=True,retrain=False,ckpt='checkpoint.pt'):\n",
    "  # train_losses =[ ]\n",
    "  valid_losses =[ ]\n",
    "  # avg_train_losses=[]\n",
    "  # avg_valid_losses=[]\n",
    "  early_stopping=EarlyStopping(patience=patience,verbose=True,\n",
    "                               path=os.path.join(path,'ckpt','checkpoint.pt'))\n",
    "  wandb.watch(model,criterion,log=\"all\",log_freq=10)\n",
    "  example_ct = 0  # number of examples seen\n",
    "  batch_ct = 0\n",
    "  batch_ct_val=0\n",
    "  example_ct_val=0\n",
    "  epoch=0\n",
    "  if retrain:\n",
    "    model.load_state_dict(torch.load(os.path.join(path,'ckpt',ckpt)))\n",
    "  if schedule:\n",
    "    print('scheduling learning rate......')\n",
    "    scheduler=torch.optim.lr_scheduler.StepLR(optimizer,step_size=50,gamma=1.1)\n",
    "  while(True):\n",
    "    \n",
    "    model.train()\n",
    "    for sample in train_loader:\n",
    "      loss=train_batch(sample['LR'],sample['HR'],model,\n",
    "                       optimizer,criterion,config)\n",
    "      # train_losses.append(loss.item())\n",
    "      example_ct+=len(sample['LR'])\n",
    "      batch_ct+=1\n",
    "      if ((batch_ct + 1) % 25) == 0:\n",
    "        log(loss, example_ct,epoch,\n",
    "            scheduler.get_last_lr()[0] if schedule else config.learning_rate,\n",
    "            0,0,\n",
    "            train=True)\n",
    "      if schedule:\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    for sample,u,v,HRnp in val_loader:\n",
    "      v_loss,psnr,ssim=validate_batch(sample['LR'],sample['HR'],\n",
    "                                      u,v,HRnp,model,criterion,epoch)\n",
    "      valid_losses.append(v_loss.item()) \n",
    "      example_ct_val+=len(sample['LR'])\n",
    "      batch_ct_val+=1\n",
    "      if (batch_ct_val+1)%8 == 0:\n",
    "        log(v_loss,example_ct_val,epoch,\n",
    "            scheduler.get_last_lr()[0] if schedule else config.learning_rate,\n",
    "            psnr,ssim,\n",
    "            train=False)\n",
    "    if schedule:\n",
    "      scheduler.step()\n",
    "    valid_loss = np.average(valid_losses)\n",
    "    valid_losses = []\n",
    "    early_stopping(valid_loss,model)\n",
    "    epoch+=1\n",
    "    # if early_stopping.early_stop:\n",
    "    #   print(\"Early stopping.........\")\n",
    "    #   break\n",
    "\n",
    "  model.load_state_dict(torch.load(os.path.join(path,'ckpt','checkpoint.pt')))\n",
    "  return model\n",
    "\n",
    "def validate_batch(LR,HR,u,v,hnp,model,criterion,epoch):\n",
    "  LR=LR.float()\n",
    "  HR=HR.float()\n",
    "  LR,HR=LR.to(device),HR.to(device)\n",
    "  SR=model(LR)\n",
    "  loss=criterion(SR,HR)\n",
    "  SR=SR.to('cpu')\n",
    "  HR=HR.to('cpu')\n",
    "  SR=SR.squeeze(1)\n",
    "  HR=HR.squeeze(1)\n",
    "  out_img_y=SR.detach().numpy()\n",
    "  out_HR_y=HR.detach().numpy()\n",
    "  snr=psnr(out_img_y,out_HR_y)\n",
    "  y=convert(out_img_y)\n",
    "  out_img_y=out_img_y.transpose(1,2,0)\n",
    "  out_HR_y=out_HR_y.transpose(1,2,0)\n",
    "  stsim=ssim(out_HR_y,out_img_y,multichannel=True)\n",
    "  u,v=np.asarray(u),np.asarray(v)\n",
    "  u=convert(u)\n",
    "  v=convert(v)\n",
    "  out_img=Image.merge('YCbCr',[y,u,v])\n",
    "  out_img=out_img.convert('RGB')\n",
    "  hrimg=Image.fromarray(np.asarray(hnp[0]))\n",
    "  plot(epoch,hrimg,out_img)\n",
    "  return loss,snr,stsim\n",
    "\n",
    "def psnr(SR,HR):\n",
    "  diff=np.subtract(HR,SR)\n",
    "  mse=np.mean(np.power(diff,2))\n",
    "  return -10*math.log10(mse)\n",
    "\n",
    "def convert(img):\n",
    "  img=img*255.0\n",
    "  img=img.clip(0,255)\n",
    "  if len(img.shape)==3:\n",
    "    out_channel=Image.fromarray(np.uint8(img[0]),mode='L')\n",
    "  elif len(img.shape) == 4:\n",
    "    out_channel=Image.fromarray(np.uint8(img[0,0]))\n",
    "    out_channel=out_channel.resize((out_channel.size[0]*2,\n",
    "                                    out_channel.size[1]*2),Image.BICUBIC)\n",
    "  return out_channel\n",
    "\n",
    "def train_batch(LR,HR,model,optimizer,criterion,config):\n",
    "  LR=LR.float()\n",
    "  HR=HR.float()\n",
    "  LR,HR=LR.to(device),HR.to(device)\n",
    "  SR=model(LR)\n",
    "  loss=criterion(SR,HR)\n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  nn.utils.clip_grad_norm_(model.parameters(),config.gradient_clip)\n",
    "  optimizer.step()\n",
    "  return loss\n",
    "\n",
    "\n",
    "def plot(epoch,HR,SR):\n",
    "  if ((epoch+1)%500 == 0) :\n",
    "    plt.figure(epoch+1)\n",
    "    plt.subplot(1,2,1,title='HR')\n",
    "    imgphr=plt.imshow(HR)\n",
    "    plt.subplot(1,2,2,title='SR')\n",
    "    imgpsr=plt.imshow(SR)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_log and val_log\n",
    "def log(loss,example_ct,epoch,lr,psnr,ssim,train=True):\n",
    "  loss=float(loss)\n",
    "  if train:\n",
    "    wandb.log({\"epoch_t\":epoch,\"training_loss\":loss,\"lr\":lr},step=example_ct)\n",
    "#     print(f\"Train_Loss after \"+ str(example_ct).zfill(5)+ f\" examples: {loss:.3f}\"+f\" lr:{lr:.6f}\")\n",
    "  else:\n",
    "    wandb.log({\"AVG_PSNR\":psnr,\"SSIM\":ssim,\"validation_loss\":loss,\"lr\":lr})\n",
    "    print(f\"Val_Loss after \"+ str(example_ct).zfill(5)+ f\" examples: {loss:.3f}\"+f\" lr:{lr:.6f}\"+f\" PSNR:{psnr:.4f}\"+f\" SSIM:{ssim:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init\n",
    "print('Do you want to start training?(Y/N)')\n",
    "a=input()\n",
    "if a.lower() == 'y':\n",
    "  model=model_pipeline(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 702, 1020])\n",
      "36.80428369824899\n",
      "torch.Size([1, 1, 924, 1020])\n",
      "26.236401234876666\n",
      "torch.Size([1, 1, 678, 1020])\n",
      "27.26198116579586\n",
      "torch.Size([1, 1, 672, 1020])\n",
      "32.65617174289202\n",
      "torch.Size([1, 1, 1020, 804])\n",
      "30.135707421558266\n",
      "torch.Size([1, 1, 1020, 678])\n",
      "35.272585863736026\n",
      "torch.Size([1, 1, 678, 1020])\n",
      "25.795685310083382\n",
      "torch.Size([1, 1, 678, 1020])\n",
      "31.368010780588868\n",
      "torch.Size([1, 1, 762, 1020])\n",
      "26.29437593050619\n",
      "torch.Size([1, 1, 822, 1020])\n",
      "30.399273266857758\n",
      "torch.Size([1, 1, 678, 1020])\n",
      "31.273144591776333\n",
      "torch.Size([1, 1, 678, 1020])\n",
      "29.48162238727616\n",
      "torch.Size([1, 1, 684, 1020])\n",
      "27.074382035992585\n",
      "torch.Size([1, 1, 1020, 678])\n",
      "29.83807717299049\n",
      "torch.Size([1, 1, 678, 1020])\n",
      "28.22764051427019\n",
      "torch.Size([1, 1, 678, 1020])\n",
      "33.82680203867727\n",
      "torch.Size([1, 1, 678, 1020])\n",
      "32.88919341468198\n",
      "torch.Size([1, 1, 684, 1020])\n",
      "31.4058701900517\n",
      "torch.Size([1, 1, 768, 1020])\n",
      "30.394602170747635\n",
      "torch.Size([1, 1, 696, 1020])\n",
      "38.48229196184463\n",
      "torch.Size([1, 1, 384, 1020])\n",
      "28.277148976227522\n",
      "torch.Size([1, 1, 480, 1020])\n",
      "25.25097650591171\n",
      "torch.Size([1, 1, 678, 1020])\n",
      "44.49097449850964\n",
      "torch.Size([1, 1, 678, 1020])\n",
      "35.179305864180215\n",
      "torch.Size([1, 1, 1020, 570])\n",
      "33.48875177969437\n",
      "torch.Size([1, 1, 768, 1020])\n",
      "23.805975447503243\n",
      "torch.Size([1, 1, 774, 1020])\n",
      "30.893423828965933\n",
      "torch.Size([1, 1, 996, 1020])\n",
      "34.472418853017956\n",
      "torch.Size([1, 1, 678, 1020])\n",
      "22.38663748748136\n",
      "torch.Size([1, 1, 636, 1020])\n",
      "30.538934709794276\n",
      "torch.Size([1, 1, 678, 1020])\n",
      "28.879090893139452\n",
      "torch.Size([1, 1, 678, 1020])\n",
      "34.521960290796684\n",
      "torch.Size([1, 1, 678, 1020])\n",
      "26.799273879652233\n",
      "torch.Size([1, 1, 678, 1020])\n",
      "26.207144390331614\n",
      "torch.Size([1, 1, 678, 1020])\n",
      "29.321482327359952\n",
      "torch.Size([1, 1, 768, 1020])\n",
      "33.139134564336544\n",
      "torch.Size([1, 1, 678, 1020])\n",
      "34.426667259362794\n",
      "torch.Size([1, 1, 612, 1020])\n",
      "27.027084505477738\n",
      "torch.Size([1, 1, 678, 1020])\n",
      "29.744378722805113\n",
      "torch.Size([1, 1, 678, 1020])\n",
      "36.87926927699823\n",
      "torch.Size([1, 1, 678, 1020])\n",
      "31.20150269949071\n",
      "torch.Size([1, 1, 678, 1020])\n",
      "26.74572072595506\n",
      "torch.Size([1, 1, 678, 1020])\n",
      "30.09589657350522\n",
      "torch.Size([1, 1, 1020, 1020])\n",
      "36.13760979584023\n",
      "torch.Size([1, 1, 570, 1020])\n",
      "26.441252995598035\n",
      "torch.Size([1, 1, 1020, 678])\n",
      "37.757260347617596\n",
      "torch.Size([1, 1, 678, 1020])\n",
      "33.023526264203646\n",
      "torch.Size([1, 1, 678, 1020])\n",
      "40.23734211975161\n",
      "torch.Size([1, 1, 678, 1020])\n",
      "28.9134520934778\n",
      "torch.Size([1, 1, 678, 1020])\n",
      "32.905072250093795\n",
      "torch.Size([1, 1, 684, 1020])\n",
      "27.804864247747627\n",
      "torch.Size([1, 1, 678, 1020])\n",
      "26.604381788851658\n",
      "torch.Size([1, 1, 678, 1020])\n",
      "29.92316463632633\n",
      "torch.Size([1, 1, 666, 1020])\n",
      "28.93077174393413\n",
      "torch.Size([1, 1, 576, 1020])\n",
      "37.726220044331484\n",
      "torch.Size([1, 1, 690, 1020])\n",
      "26.73916798566125\n",
      "torch.Size([1, 1, 678, 1020])\n",
      "32.21661281566584\n",
      "torch.Size([1, 1, 678, 1020])\n",
      "36.69984794813688\n",
      "torch.Size([1, 1, 1020, 822])\n",
      "35.72431098208164\n",
      "torch.Size([1, 1, 612, 1020])\n",
      "27.102882912829788\n",
      "torch.Size([1, 1, 1020, 642])\n",
      "28.625432824167127\n",
      "torch.Size([1, 1, 714, 1020])\n",
      "28.713716641852145\n",
      "torch.Size([1, 1, 792, 1020])\n",
      "22.22597691223321\n",
      "torch.Size([1, 1, 678, 1020])\n",
      "42.9881246107591\n",
      "torch.Size([1, 1, 594, 1020])\n",
      "24.139339380336224\n",
      "torch.Size([1, 1, 678, 1020])\n",
      "29.62169440079268\n",
      "torch.Size([1, 1, 660, 1020])\n",
      "34.02050353038867\n",
      "torch.Size([1, 1, 690, 1020])\n",
      "34.48449601655439\n",
      "torch.Size([1, 1, 1020, 624])\n",
      "25.942940074054896\n",
      "torch.Size([1, 1, 678, 1020])\n",
      "23.318010729390895\n",
      "torch.Size([1, 1, 1020, 678])\n",
      "32.3673164935318\n",
      "torch.Size([1, 1, 678, 1020])\n",
      "28.749382007345293\n",
      "torch.Size([1, 1, 570, 1020])\n",
      "27.20452789513297\n",
      "torch.Size([1, 1, 678, 1020])\n",
      "31.650970984999272\n",
      "torch.Size([1, 1, 678, 1020])\n",
      "25.53032270363618\n",
      "torch.Size([1, 1, 786, 1020])\n",
      "29.129415403025305\n",
      "torch.Size([1, 1, 678, 1020])\n",
      "38.88540520726592\n",
      "torch.Size([1, 1, 678, 1020])\n",
      "27.382170812815488\n",
      "torch.Size([1, 1, 678, 1020])\n",
      "28.996035177626247\n",
      "torch.Size([1, 1, 666, 1020])\n",
      "29.09557884136554\n",
      "torch.Size([1, 1, 678, 1020])\n",
      "39.6696740000697\n",
      "torch.Size([1, 1, 606, 1020])\n",
      "27.62094543791288\n",
      "torch.Size([1, 1, 582, 1020])\n",
      "28.359655893767343\n",
      "torch.Size([1, 1, 684, 1020])\n",
      "32.78536960723316\n",
      "torch.Size([1, 1, 678, 1020])\n",
      "36.12379181668135\n",
      "torch.Size([1, 1, 678, 1020])\n",
      "25.82330660340718\n",
      "torch.Size([1, 1, 678, 1020])\n",
      "33.70179378491875\n",
      "torch.Size([1, 1, 678, 1020])\n",
      "29.817427302966557\n",
      "torch.Size([1, 1, 1020, 768])\n",
      "35.925744753478966\n",
      "torch.Size([1, 1, 678, 1020])\n",
      "36.78396695483835\n",
      "torch.Size([1, 1, 768, 1020])\n",
      "33.05629948920841\n",
      "torch.Size([1, 1, 576, 1020])\n",
      "27.240944115869006\n",
      "torch.Size([1, 1, 684, 1020])\n",
      "28.598277634378846\n",
      "torch.Size([1, 1, 636, 1020])\n",
      "28.71708523547603\n",
      "torch.Size([1, 1, 678, 1020])\n",
      "33.94299339907437\n",
      "torch.Size([1, 1, 678, 1020])\n",
      "32.49567375495748\n",
      "torch.Size([1, 1, 594, 1020])\n",
      "31.86224580860676\n",
      "torch.Size([1, 1, 768, 1020])\n",
      "34.55763716041443\n",
      "torch.Size([1, 1, 768, 1020])\n",
      "32.53402634454645\n",
      "torch.Size([1, 1, 678, 1020])\n",
      "32.932834302083165\n",
      "Avg PSNR 30.994000519732626\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "snr=0\n",
    "dset='test'\n",
    "lrlist=os.listdir(os.path.join(path,dset,'LR'))\n",
    "lrlist.sort()\n",
    "hrlist=os.listdir(os.path.join(path,dset,'HR'))\n",
    "hrlist.sort()\n",
    "\n",
    "a=0\n",
    "# print('lr:',lrlist)\n",
    "# print('hr:',hrlist)\n",
    "with torch.no_grad():\n",
    "  model=SRHW().to(device)\n",
    "  model.load_state_dict(torch.load(os.path.join(path,'ckpt','checkpoint.pt')))\n",
    "  model.eval()\n",
    "  for i in range(len(lrlist)):\n",
    "        im=Image.open(os.path.join(path,dset,'LR',lrlist[i]))\n",
    "        im_hr=Image.open(os.path.join(path,dset,'HR',hrlist[i]))\n",
    "    #   imlr=im_hr.resize((im_hr.size[0]//2,im_hr.size[1]//2), Image.BICUBIC)\n",
    "        imlryuv=im.convert('YCbCr')\n",
    "        imhryuv=im_hr.convert('YCbCr')\n",
    "        im_hr_y,_,_=imhryuv.split()\n",
    "        im=imlryuv.convert('YCbCr')\n",
    "        y,u,v=im.split()\n",
    "        out_hr_y = y.resize(im_hr_y.size,Image.BICUBIC)\n",
    "        #print(y.size,u.size,v.size)\n",
    "        img_to_tensor = transforms.ToTensor()\n",
    "        inp = img_to_tensor(y).unsqueeze(0)\n",
    "        im_hr_y=img_to_tensor(im_hr_y).unsqueeze(0)\n",
    "        out_hr_y=img_to_tensor(out_hr_y).unsqueeze(0)\n",
    "        inp=inp.to(device)\n",
    "        print(inp.shape)\n",
    "        #change channels\n",
    "        SR=model(inp)\n",
    "        SR.squeeze(0)\n",
    "        out_y = SR.to('cpu')\n",
    "        out_y =np.asarray(out_y)\n",
    "        out_hr_y=np.asarray(out_hr_y)\n",
    "        im_hr_y=np.asarray(im_hr_y)\n",
    "#         print(out_y.shape,im_hr_y.shape)\n",
    "        out_y= out_y*a + out_hr_y*(1-a)\n",
    "        met=psnr(out_y,im_hr_y)\n",
    "        print(met)\n",
    "        snr+=met\n",
    "#         out_y *= 255.0\n",
    "#         out_y = out_y.clip(0, 255)\n",
    "#         out_y = Image.fromarray(np.uint8(out_y[0,0]),mode='L')\n",
    "#         out_img_cb = u.resize(out_y.size, Image.BICUBIC)\n",
    "#         out_img_cr = v.resize(out_y.size, Image.BICUBIC)\n",
    "#         out_img = Image.merge('YCbCr', [out_y, out_img_cb, out_img_cr])\n",
    "#         out_img=out_img.convert('RGB')\n",
    "#         out_img.save('image.png')\n",
    "\n",
    "print('Avg PSNR',snr/len(hrlist))\n",
    "print(len(hrlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 1080, 1920])\n",
      "image saved\n"
     ]
    }
   ],
   "source": [
    "a=1\n",
    "# print('lr:',lrlist)\n",
    "# print('hr:',hrlist)\n",
    "with torch.no_grad():\n",
    "  model=SRHW().to(device)\n",
    "  model.load_state_dict(torch.load(os.path.join(path,'ckpt','checkpoint.pt')))\n",
    "  model.eval()\n",
    "#   imlr=Image.open(os.path.join('/home/htic/index.jpeg'))\n",
    "  im_hr=Image.open(os.path.join('/home/htic/4k.jpg'))\n",
    "  imlr=im_hr.resize((im_hr.size[0]//2,im_hr.size[1]//2), Image.BICUBIC)\n",
    "  imlryuv=imlr.convert('YCbCr')\n",
    "#   imhryuv=im_hr.convert('YCbCr')\n",
    "#   im_hr_y,_,_=imhryuv.split()\n",
    "#   im=imlryuv.convert('YCbCr')\n",
    "  y,u,v=imlryuv.split()\n",
    "  out_hr_y = y.resize((y.size[0]*2,y.size[1]*2),Image.BICUBIC)\n",
    "  #print(y.size,u.size,v.size)\n",
    "  img_to_tensor = transforms.ToTensor()\n",
    "  inp = img_to_tensor(y).unsqueeze(0)\n",
    "#   im_hr_y=img_to_tensor(im_hr_y).unsqueeze(0)\n",
    "  out_hr_y=img_to_tensor(out_hr_y).unsqueeze(0)\n",
    "  inp=inp.to(device)\n",
    "  print(inp.shape)\n",
    "  #change channels\n",
    "  SR=model(inp)\n",
    "  SR.squeeze(0)\n",
    "  out_y = SR.to('cpu')\n",
    "  out_y =np.asarray(out_y)\n",
    "  out_hr_y=np.asarray(out_hr_y)\n",
    "#   im_hr_y=np.asarray(im_hr_y)\n",
    "#         print(out_y.shape,im_hr_y.shape)\n",
    "  out_y= out_y*a + out_hr_y*(1-a)\n",
    "#   met=psnr(out_y,im_hr_y)\n",
    "#   print(met)\n",
    "#   snr+=met\n",
    "  out_y *= 255.0\n",
    "  out_y = out_y.clip(0, 255)\n",
    "  out_y = Image.fromarray(np.uint8(out_y[0,0]),mode='L')\n",
    "  out_img_cb = u.resize(out_y.size, Image.BICUBIC)\n",
    "  out_img_cr = v.resize(out_y.size, Image.BICUBIC)\n",
    "  out_img = Image.merge('YCbCr', [out_y, out_img_cb, out_img_cr])\n",
    "  out_img=out_img.convert('RGB')\n",
    "  out_img.save('image2.png')\n",
    "  print('image saved')\n",
    "\n",
    "# print('Avg PSNR',snr/len(hrlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= SRHW().to(device)\n",
    "model.load_state_dict(torch.load(os.path.join(path,'ckpt','checkpoint.pt')))\n",
    "torch.save(model.state_dict(),os.path.join(path,'ckpt','checkpoint1_1.pt'),_use_new_zipfile_serialization=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[-3.7673e-02,  5.1877e-02, -2.2276e-02],\n",
      "          [ 6.0926e-02, -8.2014e-01,  9.4718e-02],\n",
      "          [-2.5077e-02,  7.6823e-01, -6.9616e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 2.3889e-01,  3.6425e-01,  5.0051e-02],\n",
      "          [-1.8791e-02, -6.0000e-01,  2.3152e-02],\n",
      "          [ 2.1107e-01,  2.6575e-01,  1.8312e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 9.1050e-02,  2.2793e-01,  5.0284e-02],\n",
      "          [ 7.8107e-01,  1.0090e+00, -2.8041e-01],\n",
      "          [-9.5097e-02,  7.7779e-03,  4.2008e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.7408e-01,  8.9632e-02, -1.2008e-01],\n",
      "          [-7.8207e-01,  5.8046e-01,  2.0605e-01],\n",
      "          [ 8.5006e-01,  8.7285e-02,  2.5080e-01]]],\n",
      "\n",
      "\n",
      "        [[[-9.4964e-03, -3.8138e-01,  4.8101e-01],\n",
      "          [-4.6833e-01,  3.1087e-01,  2.6158e-01],\n",
      "          [ 3.7710e-02,  3.7781e-01,  1.8380e-02]]],\n",
      "\n",
      "\n",
      "        [[[-4.6267e-01, -2.7277e-01,  4.9907e-02],\n",
      "          [ 6.1029e-01,  7.8088e-01, -3.4254e-01],\n",
      "          [-7.6313e-02, -1.7453e-01,  3.6948e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.3436e-01,  6.1180e-01,  4.3700e-01],\n",
      "          [-2.0959e-01, -2.4111e-02,  2.4962e-01],\n",
      "          [ 1.4396e-01, -9.1897e-02,  1.4629e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.1095e-01, -7.3105e-02,  2.0876e-01],\n",
      "          [-2.1064e-01,  4.8522e-01, -1.9161e-02],\n",
      "          [ 1.1681e-01,  4.3897e-02, -1.1613e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 2.1620e-02,  1.4791e-01, -8.8308e-02],\n",
      "          [ 3.2455e-01, -4.1522e-01,  8.2357e-02],\n",
      "          [ 1.9161e-01,  1.6064e-01,  1.4532e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.0303e-01,  3.5696e-01, -8.5795e-02],\n",
      "          [ 5.3799e-01,  3.2242e-01,  6.3333e-03],\n",
      "          [-2.4891e-01,  5.8922e-02, -6.1937e-02]]],\n",
      "\n",
      "\n",
      "        [[[-7.4152e-02,  5.4077e-01,  1.2734e-01],\n",
      "          [-5.3542e-01,  4.2311e-01, -3.1320e-01],\n",
      "          [ 4.1733e-01, -3.0605e-01,  1.1207e-01]]],\n",
      "\n",
      "\n",
      "        [[[-6.9159e-02,  8.7429e-03, -1.7393e-02],\n",
      "          [ 2.6658e-01,  2.1833e-02, -1.3288e-02],\n",
      "          [ 1.4975e-01,  7.7620e-01,  8.3631e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.6902e-01, -1.1892e-01,  4.4817e-02],\n",
      "          [-3.1866e-01,  1.3106e+00,  1.8354e-01],\n",
      "          [ 1.5697e-02,  2.2072e-02, -5.6999e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0811e-01, -2.3502e-01,  1.3748e-01],\n",
      "          [-1.2547e-01,  9.9444e-01,  2.8662e-01],\n",
      "          [ 2.8003e-02, -2.2859e-01, -1.0169e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0259e-01,  7.9904e-01,  2.1332e-01],\n",
      "          [-8.1317e-03, -3.0276e-01,  8.8886e-02],\n",
      "          [-2.1774e-02,  6.6835e-02, -5.6001e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 2.7164e-02, -4.6660e-02,  3.6189e-02],\n",
      "          [-8.7489e-01,  9.2170e-01, -5.6135e-02],\n",
      "          [-1.3000e-02, -3.6426e-02,  4.4611e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.2532e-01,  7.4722e-02, -8.9291e-02],\n",
      "          [ 5.6903e-02,  8.0779e-01,  1.0205e-02],\n",
      "          [-3.8480e-02, -2.4571e-01, -1.0565e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 3.9907e-01, -1.2126e-01,  1.8310e-01],\n",
      "          [-5.0714e-02,  3.0462e-02,  2.3822e-01],\n",
      "          [ 1.6859e-01,  5.7108e-01,  7.1223e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 2.6374e-01,  7.0839e-01,  5.1397e-01],\n",
      "          [ 4.8607e-01, -4.7413e-01,  6.0152e-01],\n",
      "          [-1.2907e-01,  3.6182e-02,  1.1440e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 6.6521e-01,  9.0012e-01,  2.2926e-01],\n",
      "          [ 4.6191e-01, -8.3481e-01,  9.4154e-01],\n",
      "          [ 7.0736e-02,  4.0412e-01, -1.0056e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 2.7524e-01, -6.7616e-01,  3.1374e-01],\n",
      "          [ 2.5831e-02,  7.1155e-01, -5.9917e-02],\n",
      "          [-1.2717e-01,  1.4103e-01,  1.6798e-01]]],\n",
      "\n",
      "\n",
      "        [[[-2.9534e-01,  3.9698e-02,  2.1249e-01],\n",
      "          [ 2.7559e-01,  1.8283e-01, -1.0023e-01],\n",
      "          [ 5.6852e-02, -8.7126e-01,  2.3814e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 3.1630e-01,  7.6665e-01, -2.5729e-02],\n",
      "          [-2.1013e-01, -2.8384e-01,  1.8481e-01],\n",
      "          [ 1.4335e-01,  5.0019e-02,  1.3105e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 7.9056e-01,  1.9631e-01,  4.5262e-01],\n",
      "          [-1.4507e-01, -2.5125e-01,  5.0384e-02],\n",
      "          [-4.3934e-02,  3.5085e-01,  9.9312e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.7842e-01,  3.9315e-01,  2.9530e-01],\n",
      "          [ 3.7706e-01, -3.6879e-01, -2.9929e-01],\n",
      "          [-1.3571e-01,  3.3011e-01,  1.7292e-01]]],\n",
      "\n",
      "\n",
      "        [[[-2.3753e-01, -2.6048e-01,  1.6942e-01],\n",
      "          [ 4.7396e-01, -5.2253e-01,  5.7955e-02],\n",
      "          [-1.2739e-01,  6.2013e-01, -1.7751e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.2048e-01, -1.6018e-01,  2.9482e-01],\n",
      "          [ 2.5277e-01,  3.4227e-01,  2.3018e-01],\n",
      "          [-5.9215e-02, -3.0860e-01,  2.3268e-01]]],\n",
      "\n",
      "\n",
      "        [[[-8.4582e-02,  1.3049e-01, -1.4027e-01],\n",
      "          [ 7.7250e-02,  5.7500e-01, -1.5351e-01],\n",
      "          [-1.6951e-01,  1.4655e-01, -2.4138e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 2.4795e-01,  1.0226e-01,  3.3479e-01],\n",
      "          [ 2.0064e-01,  1.5363e-03,  5.5834e-01],\n",
      "          [ 2.0070e-01,  1.3574e-01,  1.4967e-01]]],\n",
      "\n",
      "\n",
      "        [[[-8.4768e-02,  3.7881e-01,  2.0392e-01],\n",
      "          [ 8.9057e-02,  8.9559e-01, -9.9632e-06],\n",
      "          [ 2.4577e-02, -1.4005e-01, -5.0221e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 4.3466e-03, -4.6357e-01,  1.6250e-01],\n",
      "          [ 1.7894e-01,  7.7569e-01, -5.6831e-02],\n",
      "          [-2.3058e-01,  3.6432e-02, -3.6068e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.6122e-01, -1.6137e-01,  1.4955e-01],\n",
      "          [ 5.6117e-02,  3.1730e-01,  4.0123e-02],\n",
      "          [ 1.6493e-01,  4.8770e-01, -1.8014e-01]]]], device='cuda:0',\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.Conv1.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear cache\n",
    "#https://discuss.pytorch.org/t/how-can-we-release-gpu-memory-cache/14530/27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
